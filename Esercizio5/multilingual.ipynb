{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To install the required library."
      ],
      "metadata": {
        "id": "WV1kpI4EEHFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install requests\n",
        "!pip install matplotlib\n",
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dtyrekRDi6k",
        "outputId": "4c10dc4c-d410-4a45-d5c6-7b0093f97617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzsy0yet8ezy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import concurrent.futures\n",
        "import logging\n",
        "import os\n",
        "from functools import partial\n",
        "from typing import Dict, List, Optional, Set, Tuple, Any\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_U9JpHe8ez1"
      },
      "outputs": [],
      "source": [
        "from dotenv import find_dotenv, load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to setup the logging: print useful **information** and **warnings/errors**."
      ],
      "metadata": {
        "id": "jayeIC5aEWCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B2yGA108ez2"
      },
      "outputs": [],
      "source": [
        "def setup_logging() -> None:\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to setup the envinronment: used for the **BabelNet API**, the **file that contain the words** and the **languages selected**."
      ],
      "metadata": {
        "id": "ZXlOiRakEh5m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3PqkflO8ez3"
      },
      "outputs": [],
      "source": [
        "def check_dotenv(dotenv_path: Optional[str]) -> None:\n",
        "    if dotenv_path:\n",
        "        load_dotenv(dotenv_path)\n",
        "        logging.info(f\"Loaded environment variables from: {dotenv_path}\")\n",
        "    else:\n",
        "        logging.error(\"No .env file found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts the lemma for a given synset ID and language from a list of synset data."
      ],
      "metadata": {
        "id": "CgbA6XqxFuoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lemma_for_lang(\n",
        "    synsets: List[Dict[str, Any]],\n",
        "    synset_id: str,\n",
        "    lang: str\n",
        ") -> str:\n",
        "    for synset in synsets:\n",
        "        props = synset.get('properties', {})\n",
        "        sid = props.get('synsetID', {}).get('id')\n",
        "        language = props.get('language', '').upper()\n",
        "        if sid == synset_id and language == lang.upper():\n",
        "            return props.get('fullLemma') or props.get('simpleLemma') or \"N/A\"\n",
        "    return \"N/A\""
      ],
      "metadata": {
        "id": "Mm3kGIOP-N1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to load the tuple from a file."
      ],
      "metadata": {
        "id": "mXOLFCXswifu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXDczAh28ez3"
      },
      "outputs": [],
      "source": [
        "def load_word_tuples(filepath: str) -> List[Tuple[str, ...]]:\n",
        "    tuples = []\n",
        "    try:\n",
        "        with open(filepath, newline='', encoding='utf-8') as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                words = tuple(word.strip() for word in row if word.strip())\n",
        "                if len(words) >= 2:\n",
        "                    tuples.append(words)\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Input file not found: {filepath}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading input file: {e}\")\n",
        "        raise\n",
        "    return tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that create a dictionary with the synset obtained from BabelNet."
      ],
      "metadata": {
        "id": "xh6RwBoSwEx_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY7XQkad8ez4"
      },
      "outputs": [],
      "source": [
        "def find_synset_language_dict(synsets: List[dict]) -> Dict[str, Set[str]]:\n",
        "    lang_synsets: Dict[str, Set[str]] = {}\n",
        "    for synset in synsets:\n",
        "        props = synset.get('properties', {})\n",
        "        synset_id = props.get('synsetID', {}).get('id')\n",
        "        lang = props.get('language', '').upper()\n",
        "        if synset_id and lang:\n",
        "            lang_synsets.setdefault(lang, set()).add(synset_id)\n",
        "    return lang_synsets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that use the BabelNet API to retrieve the sense of a certain number of `targetLang`."
      ],
      "metadata": {
        "id": "7dtgOSPzvggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sense(\n",
        "    lemma: str,\n",
        "    targetLang: List[str],\n",
        "    key: str,\n",
        "    source: str = \"WIKI\"\n",
        ") -> Optional[List[Dict[str, Any]]]:\n",
        "    url = 'https://babelnet.io/v9/getSenses'\n",
        "    searchLang = targetLang[0]\n",
        "    params = {\n",
        "        'lemma': lemma,\n",
        "        'searchLang': searchLang,\n",
        "        'targetLang': targetLang,\n",
        "        'key': key,\n",
        "        'source': source\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params, timeout=10)\n",
        "    try:\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error fetching synsets: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "d62OyTVzFAm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function to process the tuple of words and calculate the Ambiguity Reduction Score (ARS). It follows the formula described in my paper: $$\n",
        "\\text{AmbiguityReduction} = \\frac{\\sum_{i=1}^N |\\mathcal{S}_i| - N \\cdot \\left|\\bigcap_{i=1}^N \\mathcal{S}_i\\right|}{\\sum_{i=1}^N |\\mathcal{S}_i|}\n",
        "$$\n",
        "where:\n",
        "- $ |\\bigcap_{i=1}^N \\mathcal{S}_i| \\cdot N $ are the senses shared in all the languages.  \n",
        "- $ \\sum_{i=1}^N |\\mathcal{S}_i| $ are the total number of senses across all the languages.\n",
        "\n"
      ],
      "metadata": {
        "id": "HJVYTpWLuS7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AgXhd658ez6"
      },
      "outputs": [],
      "source": [
        "def process_word_tuple(words: Tuple[str, ...], langs: List[str], api_key: str\n",
        "                       ) -> Optional[dict]:\n",
        "    if len(words) != len(langs):\n",
        "        logging.error(f\"Word tuple and language list length mismatch: {words}, {langs}\")\n",
        "        return None\n",
        "\n",
        "    synsets = get_sense(words[0], langs, api_key)\n",
        "    if not synsets:\n",
        "        logging.warning(f\"No synsets found for words: {words}\")\n",
        "        return None\n",
        "\n",
        "    lang_synsets = find_synset_language_dict(synsets)\n",
        "    synsets_sets = [lang_synsets.get(lang.upper(), set()) for lang in langs]\n",
        "\n",
        "    if not all(synsets_sets):\n",
        "        logging.info(f\"Missing synsets for some languages in {words}\")\n",
        "\n",
        "    common_synsets = set.intersection(*synsets_sets) if synsets_sets else set()\n",
        "\n",
        "    common_len = len(common_synsets)\n",
        "    langs_len = len(langs)\n",
        "    total_synsets_count = sum(len(s) for s in synsets_sets)\n",
        "\n",
        "    if total_synsets_count == 0:\n",
        "        ambiguity_reduction = 0.0\n",
        "    else:\n",
        "        ambiguity_reduction = (total_synsets_count - (common_len * langs_len)) / total_synsets_count\n",
        "\n",
        "    pseudoword = '-'.join(words)\n",
        "    save_pseudoword_multi(pseudoword, words, synsets, common_synsets, langs)\n",
        "\n",
        "    return {\n",
        "        'pseudoword': pseudoword,\n",
        "        'ambiguity_reduction': round(ambiguity_reduction, 3)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper of save_pseudoword."
      ],
      "metadata": {
        "id": "O3kBuUWgDQqy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNFPey0A8ez5"
      },
      "outputs": [],
      "source": [
        "def save_pseudoword_multi(pseudoword: str, words: Tuple[str, ...], synsets: List[dict],\n",
        "                          common_synsets: Set[str], langs: List[str]) -> None:\n",
        "    save_pseudoword(pseudoword, '-'.join(words), synsets, common_synsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function to save all the sense in common of the pseudowords generated."
      ],
      "metadata": {
        "id": "ytFScZdUCux0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pseudoword(\n",
        "    en_word: str,\n",
        "    it_word: str,\n",
        "    synsets: List[Dict[str, Any]],\n",
        "    common_synsets: Set[str]\n",
        ") -> None:\n",
        "    filename = f'rsrc/pseudowords_{en_word}_{it_word}.csv'\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['pseudoword', 'en_word', 'it_word',\n",
        "                        'en_sense', 'it_sense', 'common_synset_id'])\n",
        "\n",
        "        for synset_id in common_synsets:\n",
        "            en_sense = extract_lemma_for_lang(synsets, synset_id, 'EN')\n",
        "            it_sense = extract_lemma_for_lang(synsets, synset_id, 'IT')\n",
        "            pseudoword = f\"{en_word}-{it_word}\"\n",
        "            writer.writerow([pseudoword, en_word, it_word,\n",
        "                            en_sense, it_sense, synset_id])"
      ],
      "metadata": {
        "id": "2UBut0tO-Wcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function to save all the ambiguity reduction scores of the pseudowords generated."
      ],
      "metadata": {
        "id": "dJO1Y_fPCedo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ambiguities(\n",
        "    data: List[Dict[str, Any]],\n",
        "    filename: str = 'rsrc/ambiguity_scores.json'\n",
        ") -> None:\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "RZa3i_-d-ZZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function to plot the result in a bar chart for study purpouse."
      ],
      "metadata": {
        "id": "zRsynwe9CQL5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECC3Y1bF8ez7"
      },
      "outputs": [],
      "source": [
        "def plot_results(ambiguity_scores: List[dict]) -> None:\n",
        "    if not ambiguity_scores:\n",
        "        logging.warning(\"No data available for plotting.\")\n",
        "        return\n",
        "    pseudowords = [result['pseudoword'] for result in ambiguity_scores]\n",
        "    scores = [result['ambiguity_reduction'] for result in ambiguity_scores]\n",
        "    sorted_indices = np.argsort(scores)\n",
        "    sorted_pseudowords = [pseudowords[i] for i in sorted_indices]\n",
        "    sorted_scores = [scores[i] for i in sorted_indices]\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    y_pos = np.arange(len(sorted_pseudowords))\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_pseudowords)))\n",
        "    bars = plt.barh(y_pos, sorted_scores, color=colors)\n",
        "    plt.yticks(y_pos, sorted_pseudowords)\n",
        "    plt.xlabel('Ambiguity Reduction Score')\n",
        "    plt.title('Pseudoword Ambiguity Reduction Results')\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
        "                 f'{width:.2f}',\n",
        "                 ha='left', va='center')\n",
        "    plt.tight_layout()\n",
        "    plot_filename = 'ambiguity_reduction_plot.png'\n",
        "    plt.savefig(plot_filename)\n",
        "    logging.info(f\"Saved plot as {plot_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A wrapper to handle possible exceptions."
      ],
      "metadata": {
        "id": "xFAG2iOzCNeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NE3KgMr8ez7"
      },
      "outputs": [],
      "source": [
        "def process_word_tuple_wrapper(words: Tuple[str, ...], langs: List[str], api_key: str\n",
        "                               ) -> Optional[dict]:\n",
        "    try:\n",
        "        return process_word_tuple(words, langs, api_key)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {words}: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main: it set the *env* and the *logger*. It use a *pool of threads* to speed up the process and at the end save the result and the bar chart."
      ],
      "metadata": {
        "id": "YVVJjGmm-e1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGfPmKoF8ez7"
      },
      "outputs": [],
      "source": [
        "def main() -> None:\n",
        "    setup_logging()\n",
        "    dotenv_path = find_dotenv()\n",
        "    check_dotenv(dotenv_path)\n",
        "    API_KEY = os.getenv('BABELNET_API_KEY')\n",
        "    input_file = os.getenv('WORD_PAIRS')\n",
        "    langs_env = os.getenv('LANGUAGES')\n",
        "    if not API_KEY or not input_file or not langs_env:\n",
        "        logging.error(\"Required environment variables missing\")\n",
        "        return\n",
        "    langs = [lang.strip().upper() for lang in langs_env.split(',')]\n",
        "    try:\n",
        "        word_tuples = load_word_tuples(input_file)\n",
        "        valid_tuples = [words for words in word_tuples if len(words) == len(langs)]\n",
        "        max_workers = min(4, os.cpu_count() or 1)\n",
        "        chunk_size = 50\n",
        "        ambiguity_scores = []\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            process_func = partial(process_word_tuple_wrapper,\n",
        "                                   langs=langs, api_key=API_KEY)\n",
        "            for i in range(0, len(valid_tuples), chunk_size):\n",
        "                chunk = valid_tuples[i:i + chunk_size]\n",
        "                future_to_tuple = {\n",
        "                    executor.submit(process_func, words): words\n",
        "                    for words in chunk\n",
        "                }\n",
        "                for future in concurrent.futures.as_completed(future_to_tuple):\n",
        "                    words = future_to_tuple[future]\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        if result:\n",
        "                            ambiguity_scores.append(result)\n",
        "                            logging.info(f\"Completed {words} → Score:\"\n",
        "                                         f\"{result['ambiguity_reduction']:.3f}\")\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error processing {words}: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Fatal error: {str(e)}\", exc_info=True)\n",
        "    finally:\n",
        "        save_ambiguities(ambiguity_scores)\n",
        "        plot_results(ambiguity_scores)\n",
        "        logging.info(f\"Processed {len(ambiguity_scores)}/{len(word_tuples)} tuples\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}